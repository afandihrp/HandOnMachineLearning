{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyQEqSsXd53q"
      },
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs\n",
        "\n",
        "This notebook contains the code reproductions and theoretical explanations for Chapter 15 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7oznyCwd53t"
      },
      "source": [
        "## Chapter Summary\n",
        "\n",
        "This chapter introduces **Recurrent Neural Networks (RNNs)**, a class of neural networks designed to handle sequential data. Unlike feedforward networks, RNNs have connections that point backward, allowing them to maintain a form of memory and process sequences of arbitrary lengths.\n",
        "\n",
        "Key topics covered include:\n",
        "\n",
        "* **Recurrent Neurons and Layers:** The fundamental concepts of RNNs, how a recurrent neuron and layer work, and how they are \"unrolled through time.\"\n",
        "* **Training RNNs:** Understanding how to train RNNs using **Backpropagation Through Time (BPTT)**.\n",
        "* **Time Series Forecasting:** We build several RNNs to predict future values in a generated time series, including simple RNNs, deep RNNs, and models that can forecast multiple time steps ahead.\n",
        "* **Handling Long Sequences:** We explore the two main challenges of training RNNs on long sequences:\n",
        "    1.  **Unstable Gradients:** The vanishing/exploding gradient problem, which can be mitigated with techniques like gradient clipping, Layer Normalization, and recurrent dropout.\n",
        "    2.  **Limited Short-Term Memory:** The inability of simple RNNs to capture long-term dependencies.\n",
        "* **Advanced RNN Cells:** We introduce two powerful cell architectures that solve the short-term memory problem: **Long Short-Term Memory (LSTM)** and **Gated Recurrent Unit (GRU)** cells.\n",
        "* **CNNs for Sequences:** We explore using 1D convolutional layers to process sequences, which can be much faster than RNNs. This culminates in the **WaveNet** architecture, which uses dilated 1D convolutions to efficiently learn very long-term patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtCWuMfVd53v"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V_-mN_Rzd53w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Common setup for plotting\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHThdI20d53x"
      },
      "source": [
        "## Recurrent Neurons and Layers\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "A **recurrent neuron** is a neuron that receives not only inputs but also its own output from the previous time step. This creates a loop, allowing the network to have a form of memory. At time step *t*, the recurrent neuron receives the inputs **x**(t) and its own previous output **y**(t-1) to produce the current output **y**(t).\n",
        "\n",
        "When a layer of recurrent neurons is built, this logic is vectorized. The output of the entire layer at time step *t*, **Y**(t), is a function of the input matrix **X**(t) and the layer's output from the previous time step, **Y**(t-1). This can be visualized by **unrolling the network through time**, creating a deep network where each \"layer\" represents a time step.\n",
        "\n",
        "#### Memory Cells\n",
        "A component that preserves some state across time steps is called a **memory cell** (or just a cell). A simple recurrent neuron is a very basic cell. Its hidden state **h**(t) (which, in a simple RNN, is the same as its output **y**(t)) is a function of the previous state **h**(t-1) and the current input **x**(t).\n",
        "\n",
        "#### Input and Output Sequences\n",
        "RNNs can handle various types of input/output sequences:\n",
        "* **Sequence-to-sequence:** Input a sequence, output a sequence (e.g., forecasting stock prices).\n",
        "* **Sequence-to-vector:** Input a sequence, output a single vector (e.g., sentiment analysis of a review).\n",
        "* **Vector-to-sequence:** Input a single vector, output a sequence (e.g., image captioning).\n",
        "* **Encoder-Decoder:** A sequence-to-vector (encoder) network followed by a vector-to-sequence (decoder) network (e.g., machine translation).\n",
        "\n",
        "#### Training RNNs\n",
        "To train an RNN, we use **Backpropagation Through Time (BPTT)**. This involves unrolling the RNN through time (for the length of the input sequences) and then using regular backpropagation. The key is that the weights (**W**x and **W**y) are shared across all time steps. Gradients are computed at each time step and then summed up to update the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9494KpGd53y"
      },
      "source": [
        "## Forecasting a Time Series\n",
        "\n",
        "Let's create a function to generate some time series data. Each series will be the sum of two sine waves plus some noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8WuY-4M8d53z"
      },
      "outputs": [],
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# When dealing with time series, inputs are generally 3D:\n",
        "# [batch size, time steps, dimensionality]\n",
        "# Here, dimensionality is 1 (univariate time series)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ6PORFOd53z",
        "outputId": "e0704406-01b7-4199-aa28-f4757565fd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (7000, 50, 1)\n",
            "y_train shape: (7000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Create the datasets\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "# X_train will be the first 50 time steps, y_train will be the 51st.\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfcatwoVd530"
      },
      "source": [
        "### Baseline Metrics\n",
        "\n",
        "Before building a complex model, it's crucial to establish baseline metrics. If our RNN can't beat these, it's not very useful.\n",
        "\n",
        "1.  **Naive Forecasting:** Predict the last observed value. This is surprisingly hard to beat for some series.\n",
        "2.  **Simple Linear Model:** A fully connected network (Dense layers) that looks at all 50 time steps to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCbBgqi7d531",
        "outputId": "88cb34d0-b008-4517-afde-7c6f986dfbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Forecasting MSE: tf.Tensor(0.02153162, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# 1. Naive Forecasting\n",
        "y_pred_naive = X_valid[:, -1]\n",
        "naive_mse = tf.reduce_mean(tf.square(y_valid - y_pred_naive))\n",
        "print(\"Naive Forecasting MSE:\", naive_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDV4kyiyd531",
        "outputId": "277880f2-d5b1-44d8-e6a2-24baaa4cdd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Model MSE: 0.004609603434801102\n"
          ]
        }
      ],
      "source": [
        "# 2. Simple Linear Model\n",
        "model_linear = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[50, 1]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_linear.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_linear = model_linear.fit(X_train, y_train, epochs=20,\n",
        "                                validation_data=(X_valid, y_valid),\n",
        "                                verbose=0)\n",
        "\n",
        "linear_mse = model_linear.evaluate(X_valid, y_valid, verbose=0)\n",
        "print(\"Linear Model MSE:\", linear_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j-CDd1xd532"
      },
      "source": [
        "### Implementing a Simple RNN\n",
        "\n",
        "Now let's build the simplest possible RNN. It has a single layer with a single neuron. We don't need to specify the length of the input sequences, so we set the time step dimension to `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjOq9sxNd532",
        "outputId": "bd79609b-ab89-4763-d8d5-992b33d03575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple RNN MSE: 0.014739658683538437\n"
          ]
        }
      ],
      "source": [
        "model_simple_rnn = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "model_simple_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_simple_rnn = model_simple_rnn.fit(X_train, y_train, epochs=20,\n",
        "                                        validation_data=(X_valid, y_valid),\n",
        "                                        verbose=0)\n",
        "\n",
        "simple_rnn_mse = model_simple_rnn.evaluate(X_valid, y_valid, verbose=0)\n",
        "print(\"Simple RNN MSE:\", simple_rnn_mse)\n",
        "\n",
        "# It's better than naive, but worse than the linear model.\n",
        "# This is because it only has 3 parameters and has to preserve state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2pxA_7Rd533"
      },
      "source": [
        "### Deep RNNs\n",
        "\n",
        "We can stack multiple RNN layers to create a **Deep RNN**. This is generally more powerful.\n",
        "\n",
        "**Important:** All recurrent layers *except the last one* must have `return_sequences=True`. This ensures they output a 3D sequence (including the time step dimension) for the next recurrent layer to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x6S5KS6d533",
        "outputId": "33bf6ba7-22f0-45be-a322-eb806f1683b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep RNN MSE: 0.003031549509614706\n"
          ]
        }
      ],
      "source": [
        "model_deep_rnn = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1) # Only returns the last output\n",
        "])\n",
        "\n",
        "model_deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_deep_rnn = model_deep_rnn.fit(X_train, y_train, epochs=20,\n",
        "                                    validation_data=(X_valid, y_valid),\n",
        "                                    verbose=0)\n",
        "\n",
        "deep_rnn_mse = model_deep_rnn.evaluate(X_valid, y_valid, verbose=0)\n",
        "print(\"Deep RNN MSE:\", deep_rnn_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71hj-GOd534"
      },
      "source": [
        "The last layer doesn't need to be an RNN. A `Dense` layer is often faster, just as accurate, and allows us to choose any activation function. Here, we make the last `SimpleRNN` layer return only its last output (by setting `return_sequences=False`, which is the default) and add a `Dense` layer on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaHChJQ8d534",
        "outputId": "38a92ce2-7bac-4b33-fb66-03c317604d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep RNN with Dense output MSE: 0.0026087926235049963\n"
          ]
        }
      ],
      "source": [
        "model_deep_dense = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20), # No return_sequences=True, so it returns [batch_size, units]\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_deep_dense.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_deep_dense = model_deep_dense.fit(X_train, y_train, epochs=20,\n",
        "                                        validation_data=(X_valid, y_valid),\n",
        "                                        verbose=0)\n",
        "\n",
        "deep_dense_mse = model_deep_dense.evaluate(X_valid, y_valid, verbose=0)\n",
        "print(\"Deep RNN with Dense output MSE:\", deep_dense_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNRVHldd535"
      },
      "source": [
        "### Forecasting Several Time Steps Ahead\n",
        "\n",
        "What if we want to predict the next 10 values, not just one?\n",
        "\n",
        "**Option 1: Iterative Forecasting**\n",
        "Use the previous model, predict one step, add that prediction to the input, and call the model again to predict the next step, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rob4q3O8d535",
        "outputId": "37ca60ed-9be9-481d-87b3-a3e59f67f90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "(1, 10, 1)\n",
            "tf.Tensor(0.002272064, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Generate a new series for this test\n",
        "series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
        "\n",
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model_deep_dense.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:]\n",
        "print(Y_pred.shape)\n",
        "print(tf.reduce_mean(tf.square(Y_new - Y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MXq6BL9d536"
      },
      "source": [
        "This approach works, but errors tend to accumulate. The predictions for later time steps get progressively worse.\n",
        "\n",
        "**Option 2: Multi-Output Model (Sequence-to-Vector)**\n",
        "Train an RNN to predict all 10 next values at once. The output layer will be a `Dense` layer with 10 units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmN5mVBmd536",
        "outputId": "87c629b3-fde1-4bcf-ddfc-6be4b0893bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq-to-Vec RNN MSE: 0.008170127868652344\n"
          ]
        }
      ],
      "source": [
        "# Prepare new targets: Y_train is now [batch_size, 10]\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
        "\n",
        "model_seq_to_vec = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(10) # Output 10 values\n",
        "])\n",
        "\n",
        "model_seq_to_vec.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_seq_to_vec = model_seq_to_vec.fit(X_train, Y_train, epochs=20,\n",
        "                                        validation_data=(X_valid, Y_valid),\n",
        "                                        verbose=0)\n",
        "seq_to_vec_mse = model_seq_to_vec.evaluate(X_valid, Y_valid, verbose=0)\n",
        "print(\"Seq-to-Vec RNN MSE:\", seq_to_vec_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIK794iod537"
      },
      "source": [
        "**Option 3: Sequence-to-Sequence Model**\n",
        "\n",
        "A more powerful approach is to train the model to forecast the next 10 values at *every* time step.\n",
        "\n",
        "At time *t*, the model outputs a vector of forecasts for *t+1* to *t+10*. At time *t+1*, it outputs forecasts for *t+2* to *t+11*, etc.\n",
        "\n",
        "This helps the model because the loss is calculated at every time step, so more gradients can flow through the network, stabilizing and speeding up training.\n",
        "\n",
        "To do this, we use `return_sequences=True` on *all* recurrent layers and wrap the final `Dense` layer in a `TimeDistributed` layer. This applies the `Dense` layer at every time step independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "59lwL51vd537"
      },
      "outputs": [],
      "source": [
        "# Prepare targets: Y is now [batch_size, time_steps, 10]\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
        "\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pMQXS9kd538",
        "outputId": "dfe2cb62-f916-4dc8-beba-417ea98b5993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq-to-Seq RNN loss (full): 0.01889195293188095\n",
            "Seq-to-Seq RNN MSE (last step): 0.007327883969992399\n"
          ]
        }
      ],
      "source": [
        "model_seq_to_seq = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# For evaluation, we only care about the forecast at the very last time step.\n",
        "# So we create a custom metric.\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return tf.reduce_mean(tf.square(Y_true[:, -1] - Y_pred[:, -1]))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model_seq_to_seq.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])\n",
        "\n",
        "history_seq_to_seq = model_seq_to_seq.fit(X_train, Y_train, epochs=20,\n",
        "                                        validation_data=(X_valid, Y_valid),\n",
        "                                        verbose=0)\n",
        "\n",
        "seq_to_seq_mse = model_seq_to_seq.evaluate(X_valid, Y_valid, verbose=0)\n",
        "print(\"Seq-to-Seq RNN loss (full):\", seq_to_seq_mse[0])\n",
        "print(\"Seq-to-Seq RNN MSE (last step):\", seq_to_seq_mse[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ph617fod538"
      },
      "source": [
        "## Handling Long Sequences\n",
        "\n",
        "### Fighting the Unstable Gradients Problem\n",
        "\n",
        "**Theoretical Explanation:**\n",
        "\n",
        "Training on long sequences means BPTT has to backpropagate through many time steps. This makes the RNN effectively a very deep network, making it suffer from the **unstable gradients problem** (vanishing or exploding gradients).\n",
        "\n",
        "Several solutions exist:\n",
        "* **Gradient Clipping:** Capping the gradients at a certain threshold.\n",
        "* **Nonsaturating Activation Functions:** `tanh` is often used, but `ReLU` can lead to explosions.\n",
        "* **Batch Normalization:** Cannot be used between time steps, only between recurrent layers. It's not very effective for RNNs.\n",
        "* **Layer Normalization:** A better solution for RNNs. Instead of normalizing *across the batch dimension* (like BN), it normalizes *across the features dimension*. It's computed on the fly at each time step and behaves the same during training and testing. It's typically applied *inside* the cell.\n",
        "* **Dropout:** Can be applied to the inputs (`dropout`) and to the hidden states (`recurrent_dropout`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tYo_u8Jbd539"
      },
      "outputs": [],
      "source": [
        "# Example of a custom RNN cell with Layer Normalization\n",
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
        "                                                         activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRkhhyBSd539",
        "outputId": "10202878-6ba3-437b-e376-792661074c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# We can use this custom cell with the keras.layers.RNN layer\n",
        "model_ln_rnn = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bkw4tR2d53-"
      },
      "source": [
        "### Tackling the Short-Term Memory Problem\n",
        "\n",
        "**Theoretical Explanation:**\n",
        "\n",
        "Because of the transformations the data goes through at each time step, information from early in the sequence is gradually lost. A simple RNN has a very limited short-term memory.\n",
        "\n",
        "To solve this, more complex cells with long-term memory are used.\n",
        "\n",
        "#### LSTM Cells\n",
        "The **Long Short-Term Memory (LSTM)** cell is the most popular solution. It manages two state vectors:\n",
        "* **h**(t): The short-term state (like in a simple RNN).\n",
        "* **c**(t): The long-term state.\n",
        "\n",
        "Its key idea is that it can learn what to *store* in the long-term state, what to *throw away*, and what to *read* from it. It does this using three **gates**:\n",
        "\n",
        "1.  **Forget Gate:** Decides which parts of the long-term state **c**(t-1) to erase.\n",
        "2.  **Input Gate:** Decides which parts of the candidate state **g**(t) (the \"new\" information) to add to the long-term state.\n",
        "3.  **Output Gate:** Decides which parts of the long-term state **c**(t) to read and output as the short-term state **h**(t) and the cell output **y**(t).\n",
        "\n",
        "This architecture allows information to be preserved for very long periods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x5K4bO5qd53-"
      },
      "outputs": [],
      "source": [
        "# Using LSTM in Keras is simple\n",
        "model_lstm = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLOERFfLd53_"
      },
      "source": [
        "#### GRU Cells\n",
        "The **Gated Recurrent Unit (GRU)** cell is a simplified version of the LSTM cell that often performs just as well.\n",
        "\n",
        "It merges the two state vectors (**h** and **c**) into one and uses only two gates:\n",
        "\n",
        "1.  **Update Gate (z):** Controls how much of the *previous* state to keep and how much of the *new* candidate state to add (it handles the job of both the forget and input gates).\n",
        "2.  **Reset Gate (r):** Controls how much of the previous state to *show* to the main layer that computes the candidate state.\n",
        "\n",
        "GRU is slightly more computationally efficient than LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Jc9OR8Dxd53_"
      },
      "outputs": [],
      "source": [
        "# Using GRU in Keras\n",
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gkHfUBmd53_"
      },
      "source": [
        "### Using 1D Convolutional Layers to Process Sequences\n",
        "\n",
        "**Theoretical Explanation:**\n",
        "\n",
        "RNNs are great, but they are very sequential and thus slow to train. A **1D convolutional layer** can be a great alternative. It slides several filters (kernels) across a sequence, producing a 1D feature map per filter.\n",
        "\n",
        "* It can learn short sequential patterns (no longer than the kernel size).\n",
        "* It is not recurrent, so it can be parallelized and is much faster than an RNN.\n",
        "* It can be used as a preprocessing step for an RNN to downsample the sequence (using a stride > 1), which helps the RNN learn longer-term patterns.\n",
        "\n",
        "#### WaveNet\n",
        "The **WaveNet** architecture shows that CNNs alone can handle very long sequences. It does this by stacking 1D convolutional layers with **doubling dilation rates** (1, 2, 4, 8, ...).\n",
        "\n",
        "A dilated convolution applies a filter over an area larger than its size by \"skipping\" inputs. A dilation rate of 2 means the filter is applied to every 2nd input. This allows the network's receptive field to grow *exponentially*, making it highly efficient at learning long-term dependencies.\n",
        "\n",
        "We also use `padding=\"causal\"` to ensure that the prediction at time *t* only depends on inputs from *t* or earlier (no peeking into the future)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8SA88yxd54A",
        "outputId": "7e68cf08-cf58-4fff-8102-0cd02649f25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN+RNN loss (full): 0.018182016909122467\n",
            "CNN+RNN MSE (last step): 0.008125267922878265\n"
          ]
        }
      ],
      "source": [
        "# Example of a 1D Conv layer in front of a GRU\n",
        "model_cnn_rnn = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                          input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# Note: Because the Conv1D layer uses stride=2, it downsamples the sequence by 2.\n",
        "# And because its kernel_size is 4, the first output is based on inputs 0-3.\n",
        "# This means the targets (Y_train) must be adjusted accordingly.\n",
        "model_cnn_rnn.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_cnn_rnn = model_cnn_rnn.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
        "                                    validation_data=(X_valid, Y_valid[:, 3::2]),\n",
        "                                    verbose=0)\n",
        "\n",
        "cnn_rnn_mse = model_cnn_rnn.evaluate(X_valid, Y_valid[:, 3::2], verbose=0)\n",
        "print(\"CNN+RNN loss (full):\", cnn_rnn_mse[0])\n",
        "print(\"CNN+RNN MSE (last step):\", cnn_rnn_mse[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c924xUiYd54A",
        "outputId": "199a9c2c-17ff-407c-eb77-0d76caa672fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WaveNet loss (full): 0.020734522491693497\n",
            "WaveNet MSE (last step): 0.009024088270962238\n"
          ]
        }
      ],
      "source": [
        "# Simple WaveNet-style model\n",
        "model_wavenet = keras.models.Sequential()\n",
        "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "\n",
        "for rate in (1, 2, 4, 8) * 2: # Stack two blocks of dilated layers\n",
        "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                      activation=\"relu\", dilation_rate=rate))\n",
        "\n",
        "model_wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "\n",
        "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_wavenet = model_wavenet.fit(X_train, Y_train, epochs=20,\n",
        "                                    validation_data=(X_valid, Y_valid),\n",
        "                                    verbose=0)\n",
        "\n",
        "wavenet_mse = model_wavenet.evaluate(X_valid, Y_valid, verbose=0)\n",
        "print(\"WaveNet loss (full):\", wavenet_mse[0])\n",
        "print(\"WaveNet MSE (last step):\", wavenet_mse[1])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
