{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7y--LSBqgv"
      },
      "source": [
        "# Chapter 11 – Training Deep Neural Networks\n",
        "\n",
        "This notebook contains all the code samples and solutions to the exercises in chapter 11 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.*\n",
        "\n",
        "**Assignment Instructions:**\n",
        "Per the assignment guidelines, this notebook reproduces the code from Chapter 11. It also includes theoretical explanations and summaries for each concept, as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hysxf-qBqgz"
      },
      "source": [
        "## Chapter Summary\n",
        "\n",
        "This chapter tackles the challenges of training *deep* neural networks. While Chapter 10 introduced MLPs, training them when they are very deep (e.g., 10+ layers) presents several problems. This chapter provides solutions to these problems, allowing us to build and train powerful, deep models.\n",
        "\n",
        "Key problems and solutions covered:\n",
        "\n",
        "1.  **The Vanishing/Exploding Gradients Problem:** Gradients can get smaller and smaller (vanish) or larger and larger (explode) as they backpropagate, making lower layers very hard to train. We address this with:\n",
        "    * **Weight Initialization:** Using smarter initialization like **Glorot** and **He initialization**.\n",
        "    * **Nonsaturating Activation Functions:** Replacing functions like tanh or sigmoid with **ReLU** and its variants (**Leaky ReLU, ELU, SELU**), which do not saturate for positive values.\n",
        "    * **Batch Normalization (BN):** Adding BN layers to zero-center and normalize the inputs at each layer, which dramatically stabilizes and accelerates training.\n",
        "    * **Gradient Clipping:** Clamping the gradients during backpropagation so they never exceed a threshold.\n",
        "\n",
        "2.  **Lack of Labeled Data:** Deep networks need lots of data. If we don't have enough, we can use **Transfer Learning** to reuse the lower layers of a network already trained on a similar, large dataset. We also briefly cover *unsupervised pretraining*.\n",
        "\n",
        "3.  **Slow Training:** We can speed up training significantly by using more advanced optimizers instead of regular Stochastic Gradient Descent:\n",
        "    * **Momentum optimization**\n",
        "    * **Nesterov Accelerated Gradient (NAG)**\n",
        "    * **AdaGrad, RMSProp,** and **Adam**\n",
        "\n",
        "4.  **Overfitting:** Deep networks have millions of parameters and can easily overfit. We explore powerful regularization techniques:\n",
        "    * **L1 and L2 Regularization**\n",
        "    * **Dropout** and **Alpha Dropout** (for self-normalizing networks)\n",
        "    * **Monte Carlo (MC) Dropout** for better uncertainty estimates.\n",
        "    * **Max-Norm Regularization**\n",
        "\n",
        "Finally, the chapter provides practical guidelines and default configurations for building a high-performance deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wkkiOuBqg0"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XfrC58lcBqg2"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raVza4wQBqg3"
      },
      "source": [
        "## The Vanishing/Exploding Gradients Problems\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "The **backpropagation** algorithm works by propagating the error gradient from the output layer to the input layer. As it progresses down the network, the gradients often get smaller and smaller until they are almost zero. When this happens, Gradient Descent leaves the lower layers' connection weights virtually unchanged, and training never converges to a good solution. This is the **vanishing gradients problem**.\n",
        "\n",
        "The opposite can also happen: the gradients can grow bigger and bigger until the weights become insanely large and the algorithm diverges. This is the **exploding gradients problem**.\n",
        "\n",
        "These problems arise because of the combination of the activation functions used and the weight initialization method. For example, the logistic (sigmoid) activation function saturates at 0 and 1, where its derivative is extremely close to 0. During backpropagation, this tiny gradient gets diluted as it passes through each layer, so there is nothing left for the lower layers.\n",
        "\n",
        "We will explore several solutions to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XFUrLkBqg3"
      },
      "source": [
        "### 1. Glorot and He Initialization\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "To alleviate unstable gradients, we need the signal to flow properly in both directions (forward for predictions, backward for gradients). We need the variance of the outputs of each layer to be equal to the variance of its inputs, and the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
        "\n",
        "**Glorot (or Xavier) initialization** (named after its author) proposed a practical compromise. The connection weights of each layer must be initialized randomly as described below, where $fan_{in}$ and $fan_{out}$ are the number of input and output connections for the layer (known as *fan-in* and *fan-out*).\n",
        "\n",
        "**Glorot Initialization (for tanh, logistic, softmax):**\n",
        "* Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$, where $fan_{avg} = (fan_{in} + fan_{out}) / 2$.\n",
        "* Or a uniform distribution between $-r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$.\n",
        "\n",
        "**He Initialization (for ReLU and its variants):**\n",
        "This strategy is similar but accounts for the fact that ReLU cuts all negative values.\n",
        "* Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{2}{fan_{in}}$.\n",
        "* Or a uniform distribution between $-r$ and $+r$, with $r = \\sqrt{\\frac{6}{fan_{in}}}$.\n",
        "\n",
        "By default, Keras uses Glorot initialization with a uniform distribution. We can switch to He initialization by setting `kernel_initializer=\"he_normal\"` or `\"he_uniform\"` in a layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCYUYzbUBqg3",
        "outputId": "1d29022f-165f-41f0-de00-a00f22c7e75a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Using He initialization\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USciVS1DBqg4",
        "outputId": "f3339857-58dd-4cbc-e4ab-a8413f4da866"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_1, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# If you want He init with a uniform distribution but based on fan_avg\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=he_avg_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMNov4ufBqg4"
      },
      "source": [
        "### 2. Nonsaturating Activation Functions\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "The 2010 paper by Glorot and Bengio highlighted that the vanishing gradients problem was also due to the choice of activation function (like sigmoid or tanh), which saturate and have a derivative of 0.\n",
        "\n",
        "**ReLU (Rectified Linear Unit):** `ReLU(z) = max(0, z)`\n",
        "This function is the most popular default. It doesn't saturate for positive values and is fast to compute.\n",
        "* **Problem:** ReLU suffers from the \"dying ReLUs\" problem. During training, some neurons can \"die,\" meaning they stop outputting anything other than 0 (because their weights get tweaked so the weighted sum of their inputs is always negative). When this happens, Gradient Descent can't affect them anymore because the gradient of ReLU is 0 when $z < 0$.\n",
        "\n",
        "**Leaky ReLU:** `LeakyReLU(z) = max(αz, z)`\n",
        "This is a variant of ReLU. The hyperparameter $\\alpha$ (alpha) defines how much the function \"leaks.\" It's the slope of the function for $z < 0$ and is typically set to 0.01. This small slope ensures that leaky ReLUs never die.\n",
        "\n",
        "**PReLU (Parametric Leaky ReLU):** $\\alpha$ is *learned* during training, rather than being a fixed hyperparameter.\n",
        "\n",
        "**ELU (Exponential Linear Unit):** `ELU(z) = α(exp(z) - 1)` if $z < 0$, `z` if $z ≥ 0$.\n",
        "This function outperforms other ReLU variants: training time is reduced and accuracy is higher.\n",
        "1.  It takes on negative values, which allows the unit's average output to be closer to 0, alleviating the vanishing gradients problem.\n",
        "2.  It has a non-zero gradient for $z < 0$, which avoids the dead neurons problem.\n",
        "3.  It is smooth everywhere, which helps speed up Gradient Descent.\n",
        "* **Drawback:** It is slower to compute than ReLU due to the exponential function.\n",
        "\n",
        "**SELU (Scaled ELU):**\n",
        "This is a scaled variant of ELU. If you build a network composed exclusively of a stack of dense layers, and all hidden layers use the **SELU** activation function with **LeCun normal initialization**, then the network will **self-normalize**. The output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the unstable gradients problem.\n",
        "\n",
        "**Which activation function to use?**\n",
        "In general **SELU > ELU > Leaky ReLU > ReLU > tanh > logistic**.\n",
        "* If the network architecture allows for self-normalization, **SELU** is the best choice.\n",
        "* If not, **ELU** is a great default.\n",
        "* If you care a lot about runtime latency, **Leaky ReLU** is a good compromise.\n",
        "* **ReLU** is the most used, so many libraries and hardware accelerators are optimized for it. If speed is your priority, ReLU might be the best choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VfF1j2nBqg4",
        "outputId": "d9df52ff-219b-4ba9-fb1e-e60527591802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]), # Example input layer\n",
        "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"), # Example hidden layer\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2), # alpha is the leak parameter\n",
        "    keras.layers.Dense(10, activation=\"softmax\") # Example output layer\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aA1s7ACbBqg5"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]), # Example input layer\n",
        "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"), # Example hidden layer\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.PReLU(), # alpha will be learned\n",
        "    keras.layers.Dense(10, activation=\"softmax\") # Example output layer\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K96Ut1ldBqg5"
      },
      "outputs": [],
      "source": [
        "# Code Reproduction: Using SELU for a self-normalizing network\n",
        "\n",
        "# Note: For SELU, you must use kernel_initializer=\"lecun_normal\"\n",
        "layer = keras.layers.Dense(10, activation=\"selu\",\n",
        "                           kernel_initializer=\"lecun_normal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO4BuPJwBqg5"
      },
      "source": [
        "### 3. Batch Normalization\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "**Batch Normalization (BN)** is a technique that addresses the vanishing/exploding gradients problems, and it has become one of the most-used layers in Deep Learning.\n",
        "\n",
        "The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation does the following:\n",
        "1.  **Zero-centers and normalizes** each input.\n",
        "2.  **Scales and shifts** the result using two new parameter vectors per layer: one for scaling (gamma, $\\gamma$) and the other for shifting (beta, $\\beta$).\n",
        "\n",
        "In other words, the operation lets the model learn the optimal scale and mean of each of the layer's inputs.\n",
        "\n",
        "**How it works (training):**\n",
        "The algorithm estimates each input's mean ($\\mu$) and standard deviation ($\\sigma$) *over the current mini-batch*. Then it normalizes the input: $\\hat{x}^{(i)} = (x^{(i)} - \\mu_B) / \\sqrt{\\sigma_B^2 + \\epsilon}$.\n",
        "Finally, it scales and shifts the result: $z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta$.\n",
        "\n",
        "**How it works (testing):**\n",
        "At test time, we don't have a mini-batch to compute the mean and standard deviation. Instead, the algorithm uses the *final* statistics (mean and standard deviation of all inputs) estimated during training using a moving average.\n",
        "\n",
        "**Advantages of BN:**\n",
        "* Strongly reduces the vanishing gradients problem.\n",
        "* Networks are much less sensitive to weight initialization.\n",
        "* Allows the use of much larger learning rates, speeding up training.\n",
        "* Acts as a **regularizer**, reducing the need for other regularization techniques (like dropout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vg2_uuoQBqg6"
      },
      "outputs": [],
      "source": [
        "# Code Reproduction: Implementing Batch Normalization with Keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(), # Add BN layer as the first layer\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(), # Add BN layer after the hidden layer\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(), # Add BN layer after the hidden layer\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "S6w-JxjrBqg6",
        "outputId": "95851af5-d0e1-49f1-8d0d-004daff3e0a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quY9zkmDBqg6"
      },
      "source": [
        "Note that each BN layer adds four parameters per input: $\\gamma$, $\\beta$, $\\mu$, and $\\sigma$. The last two (the moving averages) are not trainable (they are not affected by backpropagation), so Keras calls them \"non-trainable params.\"\n",
        "\n",
        "The authors of the BN paper argued for adding BN layers *before* the activation functions. To do this, you remove the activation from the hidden layer and add it as a separate layer after the BN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "alVk_exOBqg6"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), # Set use_bias=False\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIfmYDc2Bqg7"
      },
      "source": [
        "### 4. Gradient Clipping\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "A popular technique to mitigate the **exploding gradients** problem is to clip the gradients during backpropagation so they never exceed some threshold. This is called **Gradient Clipping**.\n",
        "\n",
        "In Keras, you can set the `clipvalue` or `clipnorm` argument when creating an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "inbcpG8vBqg8"
      },
      "outputs": [],
      "source": [
        "# `clipvalue` clips every component of the gradient vector to be between -1.0 and 1.0\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "\n",
        "# `clipnorm` clips the whole gradient vector if its l2 norm is greater than 1.0\n",
        "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrbKdlGBqg8"
      },
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "It is generally not a good idea to train a very large DNN from scratch. Instead, you should almost always try to find an existing neural network that accomplishes a similar task and reuse its lower layers. This technique is called **Transfer Learning**.\n",
        "\n",
        "**Why it works:**\n",
        "* It speeds up training considerably.\n",
        "* It requires significantly less training data.\n",
        "* Lower layers of a network learn general features (e.g., edges, textures), while upper layers learn task-specific features (e.g., cat ears, dog noses). For a new, similar task, the general features are likely to be useful.\n",
        "\n",
        "**How to do it (with Keras):**\n",
        "1.  Load a pretrained model (e.g., `model_A`), excluding its top output layer (`include_top=False`).\n",
        "2.  Create a new model (`model_B_on_A`) using `model_A`'s layers, and add your new output layer on top.\n",
        "3.  **Freeze** the weights of the reused layers (by setting `layer.trainable = False` for each one) to avoid wrecking them.\n",
        "4.  Compile and train the model for a few epochs. This will only train the new output layer.\n",
        "5.  **Unfreeze** the reused layers (or some of them).\n",
        "6.  Compile the model again, this time with a **much lower learning rate**.\n",
        "7.  Continue training to fine-tune the reused layers for your new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyTM8lg8Bqg9",
        "outputId": "ebc4d315-da42-413c-fb45-68f7b1c7730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.4977 - loss: 1.5771 - val_accuracy: 0.7160 - val_loss: 0.9051\n",
            "Epoch 2/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7176 - loss: 0.8620 - val_accuracy: 0.7473 - val_loss: 0.7319\n",
            "Epoch 3/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7493 - loss: 0.7225 - val_accuracy: 0.7685 - val_loss: 0.6557\n",
            "Epoch 4/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7751 - loss: 0.6531 - val_accuracy: 0.7858 - val_loss: 0.6089\n",
            "Epoch 5/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7902 - loss: 0.6083 - val_accuracy: 0.7971 - val_loss: 0.5777\n",
            "Epoch 6/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8019 - loss: 0.5772 - val_accuracy: 0.8073 - val_loss: 0.5549\n",
            "Epoch 7/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8096 - loss: 0.5541 - val_accuracy: 0.8117 - val_loss: 0.5374\n",
            "Epoch 8/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8154 - loss: 0.5360 - val_accuracy: 0.8167 - val_loss: 0.5232\n",
            "Epoch 9/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8197 - loss: 0.5214 - val_accuracy: 0.8179 - val_loss: 0.5114\n",
            "Epoch 10/10\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8235 - loss: 0.5089 - val_accuracy: 0.8185 - val_loss: 0.5015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Code Reproduction: Transfer Learning with Keras\n",
        "\n",
        "# Let's load the Fashion MNIST dataset again\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the data. We'll pretend we're training on a different task with classes 0-7.\n",
        "X_train_A, y_train_A = X_train_full[y_train_full < 8], y_train_full[y_train_full < 8]\n",
        "X_test_A, y_test_A = X_test[y_test < 8], y_test[y_test < 8]\n",
        "\n",
        "# And the new task B (e.g., shirts vs. sandals) has very little data.\n",
        "X_train_B, y_train_B = X_train_full[y_train_full >= 8], y_train_full[y_train_full >= 8]\n",
        "X_test_B, y_test_B = X_test[y_test >= 8], y_test[y_test >= 8]\n",
        "\n",
        "# Scale the data\n",
        "X_train_A = X_train_A / 255.0\n",
        "X_test_A = X_test_A / 255.0\n",
        "X_train_B = X_train_B / 255.0\n",
        "X_test_B = X_test_B / 255.0\n",
        "\n",
        "# Let's pretend we've already trained and saved model A\n",
        "tf.random.set_seed(42)\n",
        "model_A = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(8, activation=\"softmax\") # 8 classes\n",
        "])\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_A.fit(X_train_A, y_train_A, epochs=10, validation_split=0.1)\n",
        "model_A.save(\"my_model_A.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zMuCTtTBqg9",
        "outputId": "5ecb5e72-f404-4425-b262-673bd42fd51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8318 - loss: 0.4458 - val_accuracy: 0.9233 - val_loss: 0.3211\n",
            "Epoch 2/4\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9253 - loss: 0.3009 - val_accuracy: 0.9367 - val_loss: 0.2624\n",
            "Epoch 3/4\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9378 - loss: 0.2507 - val_accuracy: 0.9525 - val_loss: 0.2277\n",
            "Epoch 4/4\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9446 - loss: 0.2198 - val_accuracy: 0.9583 - val_loss: 0.2038\n",
            "Epoch 1/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9546 - loss: 0.1958 - val_accuracy: 0.9633 - val_loss: 0.1786\n",
            "Epoch 2/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1720 - val_accuracy: 0.9692 - val_loss: 0.1592\n",
            "Epoch 3/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.1535 - val_accuracy: 0.9750 - val_loss: 0.1438\n",
            "Epoch 4/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9749 - loss: 0.1388 - val_accuracy: 0.9800 - val_loss: 0.1313\n",
            "Epoch 5/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9783 - loss: 0.1268 - val_accuracy: 0.9817 - val_loss: 0.1209\n",
            "Epoch 6/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9799 - loss: 0.1167 - val_accuracy: 0.9833 - val_loss: 0.1122\n",
            "Epoch 7/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.1082 - val_accuracy: 0.9850 - val_loss: 0.1048\n",
            "Epoch 8/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.1010 - val_accuracy: 0.9858 - val_loss: 0.0984\n",
            "Epoch 9/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.0947 - val_accuracy: 0.9858 - val_loss: 0.0928\n",
            "Epoch 10/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0891 - val_accuracy: 0.9883 - val_loss: 0.0878\n",
            "Epoch 11/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0843 - val_accuracy: 0.9892 - val_loss: 0.0835\n",
            "Epoch 12/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0799 - val_accuracy: 0.9900 - val_loss: 0.0796\n",
            "Epoch 13/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0761 - val_accuracy: 0.9908 - val_loss: 0.0761\n",
            "Epoch 14/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0726 - val_accuracy: 0.9908 - val_loss: 0.0729\n",
            "Epoch 15/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0694 - val_accuracy: 0.9908 - val_loss: 0.0700\n",
            "Epoch 16/16\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0665 - val_accuracy: 0.9908 - val_loss: 0.0674\n"
          ]
        }
      ],
      "source": [
        "# 1. Load model A\n",
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "# 2. Create model B based on A's layers (reusing all but the output layer)\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # New output layer for binary task B\n",
        "\n",
        "# 3. Freeze the reused layers\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 4. Compile and train (only the new output layer will be trained)\n",
        "# Use a larger learning rate for the new layer\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "# We subtract 8 from the labels to make them 0 (Bag) or 1 (Ankle boot)\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B - 8, epochs=4,\n",
        "                            validation_split=0.1)\n",
        "\n",
        "# 5. Unfreeze the reused layers\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 6. Compile again with a very low learning rate\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "# 7. Continue training (fine-tuning)\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B - 8, epochs=16,\n",
        "                            validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N14ol-8PBqg-",
        "outputId": "e8d55fbf-1498-4ba4-fbed-b9c32fc05628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0710\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06705056130886078, 0.9915000200271606]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model_B_on_A.evaluate(X_test_B, y_test_B - 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKb_CYqMBqg-"
      },
      "source": [
        "Transfer learning works best with deep convolutional networks, as they learn more general feature detectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4UPsMOaBqg-"
      },
      "source": [
        "## Faster Optimizers\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "Training a large DNN can be painfully slow. A huge speed boost comes from using a faster optimizer than regular Gradient Descent.\n",
        "\n",
        "**1. Momentum Optimization**\n",
        "Imagine a bowling ball rolling down a gentle slope. It starts slow, but picks up *momentum* until it reaches terminal velocity. Regular GD takes small, regular steps. Momentum optimization adds a *momentum vector* **m** to the weights, which accumulates past gradients. The gradient is used for acceleration, not speed. This allows it to roll past plateaus and converge faster. A `momentum` hyperparameter (typically 0.9) acts as friction.\n",
        "\n",
        "**2. Nesterov Accelerated Gradient (NAG)**\n",
        "A small, fast-converging variant of momentum. Instead of computing the gradient at the current position, it computes the gradient slightly *ahead* in the direction of the momentum. This is slightly more accurate and helps reduce oscillations.\n",
        "\n",
        "**3. AdaGrad (Adaptive Gradient)**\n",
        "This algorithm decays the learning rate, but it does so *faster for steep dimensions* and *slower for dimensions with gentler slopes*. This is an *adaptive learning rate*. It helps point the updates more directly toward the global optimum. However, it often stops too early because the learning rate gets scaled down too much.\n",
        "\n",
        "**4. RMSProp (Root Mean Square Propagation)**\n",
        "This algorithm fixes AdaGrad's problem by accumulating only the gradients from the most *recent* iterations (using an exponential decay). It has become a very popular optimizer.\n",
        "\n",
        "**5. Adam and Nadam**\n",
        "* **Adam** (Adaptive Moment Estimation) combines the ideas of momentum optimization and RMSProp. It keeps track of an exponentially decaying average of past gradients (like momentum) and an exponentially decaying average of past *squared* gradients (like RMSProp). It is very popular and generally performs well, requiring less tuning of the learning rate.\n",
        "* **Nadam** is Adam optimization plus the Nesterov trick. It often converges slightly faster than Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "buX7r3pcBqg-"
      },
      "outputs": [],
      "source": [
        "# Code Reproduction: Using different optimizers in Keras\n",
        "\n",
        "# Momentum\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "\n",
        "# Nesterov Accelerated Gradient\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "\n",
        "# RMSProp\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "# Adam\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Nadam\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfQo0bHSBqg-"
      },
      "source": [
        "## Learning Rate Scheduling\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "Finding a good learning rate is crucial. If it's too high, training may diverge. If it's too low, it will take too long. If it's slightly too high, it may converge fast but be unstable around the optimum.\n",
        "\n",
        "A better approach than a constant learning rate is to use a **learning schedule**, which reduces the learning rate during training. This can help you start with a large learning rate (for fast convergence) and then reduce it to settle at a good solution.\n",
        "\n",
        "Common schedules include:\n",
        "* **Power scheduling:** $\\eta(t) = \\eta_0 / (1 + t/s)^c$. Drops quickly, then more slowly.\n",
        "* **Exponential scheduling:** $\\eta(t) = \\eta_0 \\times 0.1^{t/s}$. Drops by a factor of 10 every $s$ steps.\n",
        "* **Piecewise constant scheduling:** Use a constant rate for some epochs, then a smaller rate, etc.\n",
        "* **Performance scheduling:** Reduce the rate by a factor of $\\lambda$ when the validation error stops dropping.\n",
        "* **1cycle scheduling:** Increases the rate from $\\eta_0$ to $\\eta_1$ during the first half of training, then decreases it back to $\\eta_0$ during the second half. Often speeds up training considerably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yYS0pxALBqg-"
      },
      "outputs": [],
      "source": [
        "# Code Reproduction: Implementing schedules in Keras\n",
        "\n",
        "# 1. Power scheduling (decay)\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# 2. Exponential scheduling (using a callback)\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "# Then, pass callbacks=[lr_scheduler] to model.fit()\n",
        "\n",
        "# 3. Piecewise constant scheduling (using a callback)\n",
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 15:\n",
        "        return 0.005\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "lr_scheduler_piecewise = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
        "\n",
        "# 4. Performance scheduling (using a callback)\n",
        "lr_scheduler_perf = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "\n",
        "# 5. tf.keras schedules (updates at each step, not epoch)\n",
        "X_train = X_train_full # Define X_train based on existing variable\n",
        "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
        "optimizer = keras.optimizers.SGD(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9X0bUbGBqg-"
      },
      "source": [
        "## Avoiding Overfitting Through Regularization\n",
        "\n",
        "Deep neural networks have many parameters, which gives them a lot of freedom and makes them prone to overfitting. We've already seen two regularization techniques: **Batch Normalization** and **Early Stopping**. Here are a few more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuKIfiAyBqg_"
      },
      "source": [
        "### 1. ℓ1 and ℓ2 Regularization\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "You can apply $\\ell_1$ and $\\ell_2$ regularization to constrain a neural network’s connection weights.\n",
        "* **$\\ell_2$ regularization** (like Ridge) penalizes large weights and encourages smaller weights.\n",
        "* **$\\ell_1$ regularization** (like Lasso) pushes the optimizer to zero out as many weights as it can, leading to a *sparse model*.\n",
        "\n",
        "In Keras, you can apply a kernel regularizer to any layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsWbR9xIBqg_",
        "outputId": "f77ebcb3-463c-4148-ddb7-4b39b75e0ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "# We can apply a regularizer to a layer like this:\n",
        "layer = keras.layers.Dense(100, activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "# To avoid repeating all those parameters, we can use functools.partial\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                       kernel_initializer=\"glorot_uniform\",\n",
        "                       kernel_regularizer=None) # No regularization on output layer\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jut72EHMBqhB"
      },
      "source": [
        "### 2. Dropout\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "**Dropout** is one of the most popular and successful regularization techniques for deep neural networks.\n",
        "\n",
        "**The Algorithm:** At every training step, every neuron (excluding output neurons) has a probability `p` (the *dropout rate*, typically 10-50%) of being temporarily **\"dropped out.\"** This means it will be entirely ignored during this training step, but it may be active in the next step.\n",
        "\n",
        "**Why it works:**\n",
        "1.  **More Robust Neurons:** Neurons trained with dropout cannot co-adapt with their neighboring neurons. They are forced to be as useful as possible on their own. They become less sensitive to slight changes in the inputs, leading to a more robust network.\n",
        "2.  **Ensemble Effect:** At each training step, a unique network is generated. The final neural network can be seen as an averaging ensemble of all these smaller networks.\n",
        "\n",
        "**Implementation:** In Keras, you add a `keras.layers.Dropout` layer. During training, it randomly drops inputs and divides the remaining inputs by the *keep probability* ($1 - p$). After training (at test time), it does nothing at all.\n",
        "\n",
        "**Note:** If a model is overfitting, you can increase the dropout rate. If it is underfitting, you should decrease it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BfZBcDfSBqhB"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiSPX_HjBqhC"
      },
      "source": [
        "### 3. Monte Carlo (MC) Dropout\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "**MC Dropout** is a technique that uses dropout to get a better measure of the model's uncertainty.\n",
        "\n",
        "Instead of only using dropout during training, we **also activate it during inference (prediction)**. Because dropout is active, we will get a slightly different prediction every time we run it.\n",
        "\n",
        "By making multiple predictions (e.g., 100) on the same instance and averaging them, we get a **Monte Carlo estimate** that is generally more reliable than a single prediction. More importantly, we can look at the *standard deviation* of these predictions to get a measure of the model's uncertainty.\n",
        "\n",
        "To do this, we can't just call `model.predict()`. We have to call the model as a function with `training=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "C3ucUx2-BqhC"
      },
      "outputs": [],
      "source": [
        "# This code assumes a 'model' with Dropout layers has been trained\n",
        "# and 'X_test' is available.\n",
        "\n",
        "# We'll create a simple model for demonstration\n",
        "model_mc = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_mc.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "# In a real scenario, you would fit this model first.\n",
        "\n",
        "# To perform MC Dropout:\n",
        "y_probas = np.stack([model_mc(X_test, training=True)\n",
        "                     for sample in range(100)])\n",
        "\n",
        "# y_probas shape is [n_samples, n_instances, n_classes]\n",
        "# We average over the samples to get the final probabilities:\n",
        "y_proba = y_probas.mean(axis=0)\n",
        "\n",
        "# We can also get the standard deviation to measure uncertainty\n",
        "y_std = y_probas.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt9pQ7HxBqhD"
      },
      "source": [
        "### 4. Max-Norm Regularization\n",
        "\n",
        "### Theoretical Explanation\n",
        "\n",
        "**Max-Norm Regularization** is another popular technique. For each neuron, it constrains the weights $\\mathbf{w}$ of the incoming connections such that $\\|\\mathbf{w}\\|_2 \\le r$, where $r$ is the *max-norm* hyperparameter and $\\|\\cdot\\|_2$ is the $\\ell_2$ norm.\n",
        "\n",
        "It doesn't add a regularization loss. Instead, after each training step, it checks the $\\ell_2$ norm of each neuron's weight vector and rescales it if needed ($\\|\\mathbf{w}\\| = \\mathbf{w} \\frac{r}{\\|\\mathbf{w}\\|_2}$).\n",
        "\n",
        "Reducing $r$ increases the regularization and helps reduce overfitting. It can also help alleviate unstable gradients.\n",
        "\n",
        "To implement this, you set a layer's `kernel_constraint` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTpwx2WIBqhD",
        "outputId": "0a3bce0e-cff8-442b-ae9b-aa6bef906fad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_28, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                   kernel_constraint=keras.constraints.max_norm(1.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuTXKfIMBqhD"
      },
      "source": [
        "## Summary and Practical Guidelines\n",
        "\n",
        "The chapter concludes with a table of default configurations for a standard DNN. This is an excellent starting point for most problems.\n",
        "\n",
        "**Table 11-3. Default DNN configuration**\n",
        "| Hyperparameter | Default value |\n",
        "| --- | --- |\n",
        "| Kernel initializer | He initialization |\n",
        "| Activation function | ELU |\n",
        "| Normalization | None if shallow; Batch Norm if deep |\n",
        "| Regularization | Early stopping (+ ℓ2 reg. if needed) |\n",
        "| Optimizer | Momentum optimization (or RMSProp or Nadam) |\n",
        "| Learning rate schedule | 1cycle |\n",
        "\n",
        "**Table 11-4. DNN configuration for a self-normalizing net (SELU)**\n",
        "| Hyperparameter | Default value |\n",
        "| --- | --- |\n",
        "| Kernel initializer | LeCun initialization |\n",
        "| Activation function | SELU |\n",
        "| Normalization | None (self-normalization) |\n",
        "| Regularization | Alpha dropout if needed |\n",
        "| Optimizer | Momentum optimization |\n",
        "| Learning rate schedule | 1cycle |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}